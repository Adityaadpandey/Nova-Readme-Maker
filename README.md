# ğŸ“ README Generator v2.0

An intelligent, interactive README generator that deeply understands your codebase and creates comprehensive documentation through conversation.

## âœ¨ What's New in v2.0

- **Multi-Provider Support**: Use Ollama (local), OpenAI, or Claude
- **Interactive Q&A**: The AI asks smart questions about your project before generating
- **Deep Code Understanding**: Analyzes actual code logic, not just config files
- **Multi-Pass Generation**: Analyze â†’ Ask â†’ Understand â†’ Generate â†’ Refine
- **Template System**: Choose from minimal, standard, detailed, or comprehensive styles
- **Iterative Refinement**: Review drafts and request specific changes
- **Missing Info Detection**: Identifies gaps and asks you to fill them

## ğŸš€ Quick Start

```bash
# Clone this repo
git clone https://github.com/Adityaadpandey/ReadmeMaker.git
cd ReadmeMaker

# Install dependencies
pip install -e .

# Run with Ollama (default)
python run.py https://github.com/user/project

# Run with OpenAI
export OPENAI_API_KEY=sk-...
python run.py https://github.com/user/project --model gpt-4o

# Run with Claude
export ANTHROPIC_API_KEY=sk-ant-...
python run.py https://github.com/user/project --model claude-3-5-sonnet-20241022
```

## ğŸ“‹ Requirements

- Python 3.12+
- One of the following LLM providers:

| Provider | Setup | Models |
|----------|-------|--------|
| **Ollama** (local, free) | [Install Ollama](https://ollama.ai/), run `ollama pull llama3.2:latest` | llama3.2, mistral, codellama, etc. |
| **OpenAI** | Set `OPENAI_API_KEY` env var | gpt-4o, gpt-4o-mini, o1-preview |
| **Claude** | Set `ANTHROPIC_API_KEY` env var | claude-3-5-sonnet, claude-3-opus |

For cloud providers:
```bash
pip install openai      # For OpenAI
pip install anthropic   # For Claude
```

## ğŸ¯ Usage

### Basic Usage

```bash
# Interactive mode with Ollama (default)
python run.py https://github.com/user/project

# Simple mode (no questions)
python run.py https://github.com/user/project --simple
```

### Using Different Models

```bash
# Ollama models
python run.py https://github.com/user/project --model llama3.2:3b
python run.py https://github.com/user/project --model mistral
python run.py https://github.com/user/project --model codellama

# OpenAI models (auto-detected from name)
python run.py https://github.com/user/project --model gpt-4o
python run.py https://github.com/user/project --model gpt-4o-mini
python run.py https://github.com/user/project --model o1-preview

# Claude models (auto-detected from name)
python run.py https://github.com/user/project --model claude-3-5-sonnet-20241022
python run.py https://github.com/user/project --model claude-3-opus-20240229

# Explicit provider prefix
python run.py https://github.com/user/project --model openai:gpt-4o
python run.py https://github.com/user/project --model claude:claude-3-haiku-20240307
```

### API Key Options

```bash
# Via environment variable (recommended)
export OPENAI_API_KEY=sk-...
export ANTHROPIC_API_KEY=sk-ant-...

# Via command line
python run.py https://github.com/user/project --model gpt-4o --api-key sk-...
```

## ğŸ“Š README Styles

| Style | Description | Best For |
|-------|-------------|----------|
| **Minimal** | Quick start only, ~50 lines | Simple scripts, utilities |
| **Standard** | Balanced coverage, ~150 lines | Most projects |
| **Detailed** | Comprehensive with examples, ~300 lines | Complex projects |
| **Comprehensive** | Everything included, 400+ lines | Enterprise projects |
| **API** | Focused on API documentation | Backend/API projects |
| **CLI** | Focused on commands and options | CLI tools |
| **Library** | Focused on API reference | npm/pip packages |
| **Data Science** | Includes model/dataset info | ML projects |

## ğŸ”„ Generation Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 1: Clone & Analyze                                   â”‚
â”‚  - Clone repository                                         â”‚
â”‚  - Detect languages, frameworks, technologies               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 2: Deep Code Analysis                                â”‚
â”‚  - Find entry points, classes, functions                    â”‚
â”‚  - Identify routes, models, integrations                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 3: Present Findings                                  â”‚
â”‚  - Show detected technologies                               â”‚
â”‚  - Allow corrections                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 4: Interactive Q&A                                   â”‚
â”‚  - Ask about project purpose, audience, features            â”‚
â”‚  - Context-specific questions                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 5: AI Code Understanding                             â”‚
â”‚  - LLM analyzes source code                                 â”‚
â”‚  - User can correct understanding                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 6: Choose Style                                      â”‚
â”‚  - Suggest best template                                    â”‚
â”‚  - User selects preferred style                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 7: Generate README                                   â”‚
â”‚  - Create with all gathered context                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 8: Review & Refine                                   â”‚
â”‚  - Accept, refine, or regenerate                            â”‚
â”‚  - Check for missing info                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 9: Save & Cleanup                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
readme-generator/
â”œâ”€â”€ run.py                  # Quick launcher
â”œâ”€â”€ readme_generator_v2.py  # Main interactive generator
â”œâ”€â”€ model_provider.py       # Multi-provider LLM support (Ollama/OpenAI/Claude)
â”œâ”€â”€ analyzer.py             # Project analysis (technologies, frameworks)
â”œâ”€â”€ deep_analyzer.py        # Deep code analysis (functions, classes, routes)
â”œâ”€â”€ question_engine.py      # Smart question generation
â”œâ”€â”€ readme_templates.py     # README style templates
â”œâ”€â”€ prompt.py               # Prompt creation utilities
â”œâ”€â”€ docker.py               # Repository cloning
â”œâ”€â”€ main.py                 # Original simple generator
â””â”€â”€ pyproject.toml          # Project configuration
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is open source. Feel free to use and modify.

## ğŸ’¡ Tips

- **Better results**: Answer the questions thoroughly - the more context you provide, the better the README
- **Model choice**: GPT-4o and Claude-3.5-Sonnet give excellent results; Ollama is free but may be slower
- **Refinement**: Don't hesitate to use the refine option multiple times
- **Debug mode**: Use `--debug` to keep the cloned repo and see what was analyzed
